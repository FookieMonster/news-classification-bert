{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
    "if not 'bert_repo' in sys.path:\n",
    "  sys.path += ['bert_repo']\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "import os\n",
    "import modeling\n",
    "import optimization\n",
    "import tokenization\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "  def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "    \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "    self.guid = guid\n",
    "    self.text_a = text_a\n",
    "    self.text_b = text_b\n",
    "    self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingInputExample(object):\n",
    "  \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "  \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               label_id,\n",
    "               is_real_example=True):\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segment_ids = segment_ids\n",
    "    self.label_id = label_id\n",
    "    self.is_real_example = is_real_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "  \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "  def get_train_examples(self, data_dir):\n",
    "    \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_dev_examples(self, data_dir):\n",
    "    \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_test_examples(self, data_dir):\n",
    "    \"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  @classmethod\n",
    "  def _read_tsv(cls, input_file, quotechar=None):\n",
    "    \"\"\"Reads a tab separated value file.\"\"\"\n",
    "    with tf.gfile.Open(input_file, \"r\") as f:\n",
    "      reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "      lines = []\n",
    "      for line in reader:\n",
    "        lines.append(line)\n",
    "      return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JpProcessor(DataProcessor):\n",
    "  \"\"\"Processor for the Japanese data set.\"\"\"\n",
    "\n",
    "  def read_tsv(self, path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    return [(str(text), str(label)) for text,label in zip(df['text'], df['label'])]\n",
    "\n",
    "  def get_train_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self.read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "  def get_dev_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self.read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "  def get_test_examples(self, data_dir):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "      self.read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "\n",
    "  def _create_examples(self, lines, set_type):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "      guid = \"%s-%s\" % (set_type, i)\n",
    "      text_a = tokenization.convert_to_unicode(line[0])\n",
    "      label = tokenization.convert_to_unicode(line[1])\n",
    "      examples.append(\n",
    "          InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(ex_index, example, label_list, max_seq_length,\n",
    "                           tokenizer):\n",
    "  \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "  if isinstance(example, PaddingInputExample):\n",
    "    return InputFeatures(\n",
    "        input_ids=[0] * max_seq_length,\n",
    "        input_mask=[0] * max_seq_length,\n",
    "        segment_ids=[0] * max_seq_length,\n",
    "        label_id=0,\n",
    "        is_real_example=False)\n",
    "\n",
    "  label_map = {}\n",
    "  for (i, label) in enumerate(label_list):\n",
    "    label_map[label] = i\n",
    "\n",
    "  tokens_a = tokenizer.tokenize(example.text_a)\n",
    "  tokens_b = None\n",
    "  if example.text_b:\n",
    "    tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "  if tokens_b:\n",
    "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "    # length is less than the specified length.\n",
    "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "  else:\n",
    "    # Account for [CLS] and [SEP] with \"- 2\"\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "      tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "  # The convention in BERT is:\n",
    "  # (a) For sequence pairs:\n",
    "  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "  # (b) For single sequences:\n",
    "  #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "  #  type_ids: 0     0   0   0  0     0 0\n",
    "  #\n",
    "  # Where \"type_ids\" are used to indicate whether this is the first\n",
    "  # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "  # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "  # embedding vector (and position vector). This is not *strictly* necessary\n",
    "  # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "  # it easier for the model to learn the concept of sequences.\n",
    "  #\n",
    "  # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "  # used as the \"sentence vector\". Note that this only makes sense because\n",
    "  # the entire model is fine-tuned.\n",
    "  tokens = []\n",
    "  segment_ids = []\n",
    "  tokens.append(\"[CLS]\")\n",
    "  segment_ids.append(0)\n",
    "  for token in tokens_a:\n",
    "    tokens.append(token)\n",
    "    segment_ids.append(0)\n",
    "  tokens.append(\"[SEP]\")\n",
    "  segment_ids.append(0)\n",
    "\n",
    "  if tokens_b:\n",
    "    for token in tokens_b:\n",
    "      tokens.append(token)\n",
    "      segment_ids.append(1)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(1)\n",
    "\n",
    "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "  # tokens are attended to.\n",
    "  input_mask = [1] * len(input_ids)\n",
    "\n",
    "  # Zero-pad up to the sequence length.\n",
    "  while len(input_ids) < max_seq_length:\n",
    "    input_ids.append(0)\n",
    "    input_mask.append(0)\n",
    "    segment_ids.append(0)\n",
    "\n",
    "  assert len(input_ids) == max_seq_length\n",
    "  assert len(input_mask) == max_seq_length\n",
    "  assert len(segment_ids) == max_seq_length\n",
    "\n",
    "  label_id = label_map[example.label]\n",
    "  if ex_index < 5:\n",
    "    tf.logging.info(\"*** Example ***\")\n",
    "    tf.logging.info(\"guid: %s\" % (example.guid))\n",
    "    tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "        [tokenization.printable_text(x) for x in tokens]))\n",
    "    tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "    tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "    tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "    tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "  feature = InputFeatures(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids,\n",
    "      label_id=label_id,\n",
    "      is_real_example=True)\n",
    "  return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストから特徴量に変換してJSONファイルに保存（シングル）\n",
    "VOCAB_FILE = 'bert_checkpoint/multi_cased_L-12_H-768_A-12/vocab.txt'\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=False)\n",
    "\n",
    "text_a = tokenization.convert_to_unicode(\"急速充電や大容量バッテリー、エコナビ対応のXi対応ドコモスマホ「ELUGA power P-07D」の気になる価格は？\")\n",
    "label = tokenization.convert_to_unicode(\"0\")\n",
    "labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "\n",
    "example = InputExample(guid=\"\", text_a=text_a, text_b=None, label=label)\n",
    "feature = convert_single_example(0, example, labels, 128, tokenizer)\n",
    "\n",
    "json_str = '{{\"input_ids\":{0},\"input_mask\":{1},\"segment_ids\":{2},\"label_ids\":{3}}}' \\\n",
    "    .format(feature.input_ids, feature.input_mask, feature.segment_ids, [feature.label_id])\n",
    "print(json_str)\n",
    "\n",
    "with open('input-single.json', 'w') as f:\n",
    "  print(json_str, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストから特徴量に変換してJSONファイルに保存（マルチ）\n",
    "VOCAB_FILE = 'bert_checkpoint/multi_cased_L-12_H-768_A-12/vocab.txt'\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=False)\n",
    "processor = JpProcessor()\n",
    "label_list = processor.get_labels()\n",
    "\n",
    "lines = [[\"急速充電や大容量バッテリー、エコナビ対応のXi対応ドコモスマホ「ELUGA power P-07D」の気になる価格は？\", \"0\"], \\\n",
    "         [\"声に反応！　東芝の「VoiPY」で電化製品を声でコントロール【売れ筋チェック】\", \"0\"], \\\n",
    "         [\"【Sports Watch】朝青龍の協会批判は誤解、通訳者のミスだった!?\", \"0\"]]\n",
    "examples = processor._create_examples(lines, \"test\")\n",
    "\n",
    "json_str = \"\"\n",
    "for (ex_index, example) in enumerate(examples):\n",
    "    feature = convert_single_example(ex_index, example, label_list, 128, tokenizer)\n",
    "    json_str = json_str + '{{\"input_ids\":{0},\"input_mask\":{1},\"segment_ids\":{2},\"label_ids\":{3}}}' \\\n",
    "        .format(feature.input_ids, feature.input_mask, feature.segment_ids, [feature.label_id])\n",
    "    if ex_index != len(lines) -1:\n",
    "        json_str = json_str + \"\\n\"\n",
    "\n",
    "with open('input-multi.json', 'w') as f:\n",
    "  print(json_str, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"news_classification\"\n",
    "VERSION_NAME=\"v1\"\n",
    "\n",
    "# 予測のリクエスト\n",
    "!gcloud ai-platform predict \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --version $VERSION_NAME \\\n",
    "  --json-instances input-single.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
